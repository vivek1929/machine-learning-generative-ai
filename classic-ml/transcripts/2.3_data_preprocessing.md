# Data Preprocessing for Machine Learning - Session Summary

## Overview
This session covered the fundamentals of data preprocessing in machine learning, emphasizing its critical importance in the ML pipeline. The instructor stressed that while model building might be just one line of code, preprocessing constitutes approximately 40% of a data scientist's work.

## Key Philosophy: "Garbage In, Garbage Out"
- Data preprocessing is like laying a foundation for a house
- Poor preprocessing leads to complex models that may not perform well
- It's better to spend time on solid preprocessing than building complex models on poor data

## Data Science Workflow
1. **Use Case Discussion** (Senior stakeholders involved)
2. **Data Collection** (30% of work) - Understanding data sources, merging tables
3. **Data Preprocessing** (40% of work) - Today's focus
4. **Model Building** (10% of work) - Often just one line of code
5. **Iteration and Improvement** (Remaining 20%) - Based on results

## Types of Data Issues

### Data Quality Issues
These issues exist regardless of model building and need to be addressed:

1. **Missing Values**
   - Can occur due to user privacy, data collection errors, or system issues
   - Need to understand why values are missing before handling

2. **Duplicates**
   - Redundant records that can confuse model training
   - Must be identified and removed systematically

3. **Inconsistent Data**
   - Same information represented differently (e.g., "New York" vs "NY")
   - Date formats varying across entries
   - Requires standardization

4. **Outliers**
   - May be genuine outliers or data errors
   - Requires domain expertise to determine treatment
   - Context-dependent handling

5. **Wrong Data Types**
   - Numeric data interpreted as categorical or vice versa
   - Dates not recognized as datetime objects
   - Affects model performance significantly

### Model-Specific Issues
These issues specifically impact machine learning model performance:

1. **Highly Correlated Features**
   - Features that represent similar information
   - Example: Interest rates and inflation
   - Can lead to redundant information overloading the model

2. **Feature Scaling**
   - Different scales between features (e.g., age vs salary)
   - Models focus on larger numeric values, ignoring smaller but potentially important features
   - Essential for algorithms that rely on distance calculations

3. **Class Imbalance**
   - Uneven distribution of target classes
   - May or may not be problematic depending on feature strength
   - Techniques: upsampling, downsampling

4. **Multicollinearity**
   - Linear combinations of features that predict other features
   - Example: 3x₁ + 4x₂ = x₄
   - Leads to redundancy in feature space

5. **Data Leakage**
   - Inadvertent inclusion of future information in training
   - Must be carefully avoided in train-test splits

## Data Loading Techniques

### 1. CSV Files
```python
import pandas as pd
df = pd.read_csv('filename.csv')
```

### 2. Excel Files
```python
df = pd.read_excel('filename.xlsx')
```

### 3. Database Connections
```python
# Establish connection
connection = establish_db_connection()
# Query data
df = pd.read_sql(query, connection)
# Close connection
connection.close()
```

### 4. Built-in Datasets
```python
from sklearn.datasets import load_iris
iris = load_iris(as_frame=True)
df = iris.frame
```

### 5. API Data
```python
import requests
response = requests.get('api_url')
data = response.json()
df = pd.DataFrame(data)
```

### 6. Financial Data (Yahoo Finance)
```python
import yfinance as yf
df = yf.download('AAPL', start='2020-01-01', end='2023-01-01')
```

## Data Type Conversion

### Checking Data Types
```python
df.dtypes  # View all column data types
df.select_dtypes(include='object')  # Select categorical columns
```

### Converting Data Types
```python
# Convert to numeric
pd.to_numeric(df['column'], errors='coerce')

# Convert using astype
df['column'].astype('category')

# Automatic conversion
df.convert_dtypes()
```

## Initial Data Exploration

### Basic Information
```python
df.shape  # Dimensions (rows, columns)
df.columns  # Column names
df.head()  # First 5 rows
df.tail()  # Last 5 rows
df.describe()  # Summary statistics
```

### Missing Value Analysis
```python
df.isnull().sum()  # Count missing values per column
df.nunique()  # Number of unique values per column
```

### Visualization
```python
# Distribution plots
df.hist()

# Pair plots for relationships
import seaborn as sns
sns.pairplot(df)

# Advanced profiling
from pandas_profiling import ProfileReport
profile = ProfileReport(df)
```

## Duplicate Handling

### Identifying Duplicates
```python
# Check for duplicates
df.duplicated().sum()

# View duplicate rows
df[df.duplicated()]
```

### Removing Duplicates
```python
# Remove duplicate rows
df.drop_duplicates(inplace=True)

# Check shape after removal
df.shape
```

## Handling Inconsistent Data

### Text Cleaning Example
```python
# Standardize city names
df['city_clean'] = (df['city']
                   .str.lower()
                   .str.strip()
                   .map({'ny': 'new york', 'sf': 'san francisco'}))
```

### Fuzzy Matching
```python
from fuzzywuzzy import process

# Find similar strings
for city in unique_cities:
    matches = process.extract(city, unique_cities, limit=3)
    print(f"Top matches for {city}: {matches}")
```

### Extracting Numeric Values
```python
# Remove non-numeric characters
df['price_clean'] = (df['price']
                    .str.replace(r'[^0-9.]', '', regex=True)
                    .astype(float))
```

## Missing Value Strategies

### Simple Approaches
```python
# Drop rows with any missing values
df.dropna()

# Drop columns with >50% missing values
threshold = len(df) * 0.5
df.dropna(axis=1, thresh=threshold)
```

### Advanced Techniques (covered in detail in subsequent sessions)
- Mean/median/mode imputation
- Forward/backward fill
- Interpolation
- Model-based imputation
- Multiple imputation

## Best Practices and Key Takeaways

### Documentation and Communication
1. **Always document your findings** - Missing values, outliers, inconsistencies
2. **Highlight issues to stakeholders** before making changes
3. **Record all preprocessing choices** for reproducibility
4. **Maintain original data** while creating cleaned versions

### Iterative Process
- Preprocessing is **not linear** - expect to revisit steps multiple times
- Results from model building inform preprocessing decisions
- Each iteration should be documented with rationale

### Domain Expertise
- **Collaborate with Subject Matter Experts (SMEs)** for domain-specific decisions
- **Understand business context** before making preprocessing choices
- **Legal and logical validation** of preprocessing steps

### Experimentation Approach
1. **Establish baseline** with minimal preprocessing
2. **Change one step at a time** to understand impact
3. **Compare results** before and after each change
4. **Document what works** and what doesn't

### Tools and Scalability
- **Pandas**: Good for smaller datasets (MB range)
- **Spark/PySpark**: Required for larger datasets (GB/TB range)
- **SQL**: Often used for initial data processing
- **Cloud platforms**: For very large scale processing

## Common Pitfalls to Avoid

1. **Don't assume data quality** - always validate
2. **Avoid overfitting preprocessing** to training data
3. **Don't ignore domain knowledge** - statistical techniques alone aren't enough
4. **Don't skip documentation** - future you will thank present you
5. **Avoid data leakage** in preprocessing steps

## Next Steps
The session concluded with plans to continue covering:
- Advanced missing value imputation techniques
- Outlier detection and treatment methods
- Feature scaling and normalization
- Encoding categorical variables
- Feature engineering techniques

## Tools and Libraries Mentioned
- **pandas**: Primary data manipulation library
- **scikit-learn**: Built-in datasets and preprocessing tools
- **yfinance**: Financial data access
- **fuzzywuzzy**: String matching and cleaning
- **seaborn/matplotlib**: Data visualization
- **pandas-profiling**: Automated EDA reports

---

*Note: This session emphasized that preprocessing is an iterative, experience-driven process that requires domain knowledge, careful documentation, and continuous validation against business objectives.* 