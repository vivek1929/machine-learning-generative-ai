{"cells":[{"cell_type":"markdown","metadata":{"id":"H7W6IuZjFVoD"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"],"id":"H7W6IuZjFVoD"},{"cell_type":"markdown","metadata":{"id":"bGgCVs0KGzDN"},"source":["### Not for Grading"],"id":"bGgCVs0KGzDN"},{"cell_type":"markdown","id":"5d352c24","metadata":{"id":"5d352c24"},"source":["# Titanic Dataset Preprocessing Assignment\n","\n","In this assignment, we will preprocess the Titanic dataset to practice data cleaning, encoding, scaling, binarization, polynomial features, discretization, power transformation, and feature selection. We will use Scikit-learn's preprocessing functions to make the data ready for machine learning.\n","\n","### Objectives:\n","- Handle missing values\n","- Encode categorical variables\n","- Scale features\n","- Apply binarization and discretization\n","- Generate polynomial features\n","- Perform power transformation\n","- Select the top features\n"]},{"cell_type":"markdown","metadata":{"id":"BNLA8HiKxQhc"},"source":["### Setup Steps:"],"id":"BNLA8HiKxQhc"},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[],"id":"2YzfoPvJDiTX"},{"cell_type":"code","metadata":{"id":"rEzlYL4CDrmE"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[],"id":"rEzlYL4CDrmE"},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","import re\n","ipython = get_ipython()\n","\n","notebook= \"U1W2_03_Titanic_Preprocessing_sklearn\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    from IPython.display import HTML, display\n","    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Purchase_Dataset.csv\")\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","\n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n","              \"concepts\" : Concepts, \"record_id\" : submission_id,\n","               \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_inclass_mentor\": Mentor_support}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:\n","        print(r[\"err\"])\n","        return None\n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions:  https://learn-iiith.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","\n","\n","def getAdditional():\n","  try:\n","    if not Additional:\n","      raise NameError\n","    else:\n","      return Additional\n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","\n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","\n","def getWalkthrough():\n","  try:\n","    if not Walkthrough:\n","      raise NameError\n","    else:\n","      return Walkthrough\n","  except NameError:\n","    print (\"Please answer Walkthrough Question\")\n","    return None\n","\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","\n","def getId():\n","  try:\n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup\n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[],"id":"WBPPuGmBlDIN"},{"cell_type":"code","execution_count":null,"id":"ff0ca260","metadata":{"id":"ff0ca260"},"outputs":[],"source":["# Loading the dataset\n","import pandas as pd\n","url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n","data = pd.read_csv(url)\n","print(\"First 5 rows of the dataset:\\n\", data.head())\n","print(\"\\nDataset information:\")\n","data.info()"]},{"cell_type":"markdown","id":"43d57c53","metadata":{"id":"43d57c53"},"source":["## Step 2: Handling Missing Values"]},{"cell_type":"code","execution_count":null,"id":"bab8ba35","metadata":{"id":"bab8ba35"},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","# Impute numerical columns\n","imputer_num = SimpleImputer(strategy=\"median\")\n","data[\"Age\"] = imputer_num.fit_transform(data[[\"Age\"]])\n","data[\"Fare\"] = imputer_num.fit_transform(data[[\"Fare\"]])\n","print(\"\\nAfter imputing numerical columns:\\n\", data[[\"Age\", \"Fare\"]].head())\n","\n","# Impute categorical columns\n","imputer_cat = SimpleImputer(strategy=\"most_frequent\")"]},{"cell_type":"code","source":["data[\"Embarked\"] = imputer_cat.fit_transform(data[[\"Embarked\"]]).ravel()\n","print(\"\\nAfter imputing categorical column 'Embarked':\\n\", data[\"Embarked\"].head())"],"metadata":{"id":"2-NrQL_TNOLB"},"id":"2-NrQL_TNOLB","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"bea84d64","metadata":{"id":"bea84d64"},"source":["## Step 3: Encoding Categorical Variables"]},{"cell_type":"code","source":["pd.DataFrame([['A','B','C'],[]])"],"metadata":{"id":"72YGBqe_QY7f"},"id":"72YGBqe_QY7f","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4f5e9834","metadata":{"id":"4f5e9834"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Label Encoding for binary categorical column 'Sex'\n","le = LabelEncoder()\n","data[\"Sex\"] = le.fit_transform(data[\"Sex\"])\n","print(\"\\nAfter label encoding 'Sex':\\n\", data[\"Sex\"].head())\n","\n","# One-hot encoding for multi-category columns\n","data = pd.get_dummies(data, columns=[\"Embarked\", \"Pclass\"], drop_first=True)\n","print(\"\\nAfter one-hot encoding 'Embarked' and 'Pclass':\\n\", data.head())"]},{"cell_type":"markdown","id":"22f071fb","metadata":{"id":"22f071fb"},"source":["## Step 4: Feature Scaling"]},{"cell_type":"code","execution_count":null,"id":"0db1ae1d","metadata":{"id":"0db1ae1d"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","# Standard Scaling for 'Fare'\n","scaler_standard = StandardScaler()\n","print(data['Fare'].head())\n","data[\"Fare\"] = scaler_standard.fit_transform(data[[\"Fare\"]])\n","print(\"\\nAfter standard scaling 'Fare':\\n\", data[\"Fare\"].head())\n","\n","# Min-Max Scaling for 'Age'\n","scaler_minmax = MinMaxScaler()\n","data[\"Age\"] = scaler_minmax.fit_transform(data[[\"Age\"]])\n","print(\"\\nAfter min-max scaling 'Age':\\n\", data[\"Age\"].head())"]},{"cell_type":"code","source":["data[[\"Fare\"]].max()"],"metadata":{"id":"66fqZrAsSZgN"},"id":"66fqZrAsSZgN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3bf826b3","metadata":{"id":"3bf826b3"},"source":["## Step 5: Binarization"]},{"cell_type":"code","source":["scaler_minmax.inverse_transform([[0.5]])"],"metadata":{"id":"JX2M_qbAXNvR"},"id":"JX2M_qbAXNvR","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"bd73fc23","metadata":{"id":"bd73fc23"},"outputs":[],"source":["from sklearn.preprocessing import Binarizer\n","\n","binarizer = Binarizer(threshold=0.5)\n","data[\"Age_binarized\"] = binarizer.fit_transform(data[[\"Age\"]])\n","print(\"\\nAfter binarizing 'Age':\\n\", data[[\"Age\", \"Age_binarized\"]].head())"]},{"cell_type":"markdown","id":"645eebd5","metadata":{"id":"645eebd5"},"source":["## Step 6: Discretization"]},{"cell_type":"code","execution_count":null,"id":"67631b6c","metadata":{"id":"67631b6c"},"outputs":[],"source":["from sklearn.preprocessing import KBinsDiscretizer\n","[]\n","discretizer = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\")\n","data[\"Fare_binned\"] = discretizer.fit_transform(data[[\"Fare\"]])\n","print(\"\\nAfter discretizing 'Fare':\\n\", data[[\"Fare\", \"Fare_binned\"]].head())"]},{"cell_type":"code","source":["for a,b in data[[\"Fare\", \"Fare_binned\"]].groupby(\"Fare_binned\"):\n","  print(a,b)"],"metadata":{"id":"EyKgIWQ1YvgI"},"id":"EyKgIWQ1YvgI","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"db96a492","metadata":{"id":"db96a492"},"source":["## Step 7: Polynomial Features"]},{"cell_type":"code","execution_count":null,"id":"ab3b4cc4","metadata":{"id":"ab3b4cc4"},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","poly = PolynomialFeatures(degree=2, include_bias=True)\n","poly_features = poly.fit_transform(data[[\"Age\", \"Fare\"]])\n","poly_feature_names = poly.get_feature_names_out([\"Age\", \"Fare\"])\n","data[poly_feature_names] = poly_features\n","print(\"\\nAfter generating polynomial features:\\n\", data[poly_feature_names].head())"]},{"cell_type":"markdown","id":"b01e6440","metadata":{"id":"b01e6440"},"source":["## Step 8: Power Transformation"]},{"cell_type":"code","execution_count":null,"id":"27b4a8bd","metadata":{"id":"27b4a8bd"},"outputs":[],"source":["from sklearn.preprocessing import PowerTransformer\n","\n","power_transformer = PowerTransformer()\n","data[\"Fare_power\"] = power_transformer.fit_transform(data[[\"Fare\"]])\n","print(\"\\nAfter power transforming 'Fare':\\n\", data[[\"Fare\", \"Fare_power\"]].head())"]},{"cell_type":"markdown","id":"a433bfed","metadata":{"id":"a433bfed"},"source":["## Step 9: Feature Selection"]},{"cell_type":"code","execution_count":null,"id":"01c20e9c","metadata":{"id":"01c20e9c"},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest, f_classif\n","\n","# Dropping non-numeric columns\n","data_numeric = data.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\n","\n","# Define features and target\n","X = data_numeric.drop(\"Survived\", axis=1)\n","y = data_numeric[\"Survived\"]\n","\n","# Apply SelectKBest\n","selector = SelectKBest(score_func=f_classif, k=10)\n","X_new = selector.fit_transform(X, y)\n","selected_features = X.columns[selector.get_support(indices=True)]\n","print(\"\\nSelected features based on ANOVA F-value:\", selected_features)"]},{"cell_type":"markdown","id":"19e2944b","metadata":{"id":"19e2944b"},"source":["## Step 10: Splitting the Data"]},{"cell_type":"code","execution_count":null,"id":"4190b8dd","metadata":{"id":"4190b8dd"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Using the selected features\n","X = data[selected_features]  # Use only selected features for training\n","y = data[\"Survived\"]\n","\n","# Splitting into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","print(\"\\nFirst 5 rows of training set:\\n\", X_train.head())"]},{"cell_type":"markdown","source":["1. Handling Missing Values\n","\n","Purpose: Missing values can cause errors in machine learning algorithms or lead to biased models if not handled properly.\n","\n","Effect: Imputing with the median for numeric columns ensures that extreme values don’t unduly influence the imputed value. For categorical columns, using the most frequent category helps avoid creating artificial patterns by filling missing entries with a reasonable, frequently occurring value.\n","\n","Modeling Impact: Filling missing values prevents data loss and ensures that each feature can contribute effectively to the model without gaps.\n","\n","2. Encoding Categorical Variables\n","\n","Purpose: Machine learning models generally cannot handle categorical variables directly, as they require numerical inputs.\n","\n","Effect: Label encoding binary categorical features, like Sex, converts them into numerical form, while one-hot encoding for multi-class features, like Embarked and Pclass, ensures that the model doesn’t assume any ordinal relationship between categories.\n","\n","Modeling Impact: Encoding enables the model to learn from categorical features without assuming ordinal relationships, improving interpretability and performance for these variables.\n","\n","3. Feature Scaling\n","\n","Purpose: Different features in the dataset may have vastly different scales, which can lead to biased model behavior (favoring features with larger scales).\n","\n","Effect: Standard scaling (Fare) and min-max scaling (Age) ensure that all features contribute equally to the model, especially in distance-based algorithms like KNN or SVM.\n","\n","Modeling Impact: Scaling helps the model converge faster and provides a balanced treatment for all features, leading to more stable and reliable performance.\n","\n","4. Binarization\n","\n","Purpose: For some features, transforming continuous values into binary categories (e.g., Age) can be helpful for simplifying complex distributions or highlighting specific distinctions (e.g., minor vs. adult).\n","\n","Effect: Binarizing Age at a threshold of 0.5 (in the scaled data) separates passengers into two groups (young vs. old), which might provide an alternate representation of age with specific modeling insights.\n","\n","Modeling Impact: This approach can make certain features more interpretable and lead to better performance in cases where binary decisions or groups are more meaningful.\n","\n","5. Discretization\n","\n","Purpose: Discretizing continuous features into discrete bins can sometimes improve model performance by reducing noise and capturing distinct groups.\n","\n","Effect: Dividing Fare into three bins (low, medium, high) allows the model to treat fare categories distinctly rather than trying to interpret a continuous scale, which may enhance its ability to make categorical distinctions.\n","\n","Modeling Impact: Discretization can simplify complex distributions, making it easier for certain models (like tree-based models) to understand the feature’s impact.\n","\n","6. Polynomial Features\n","\n","Purpose: Polynomial features capture interactions between features and can reveal non-linear relationships.\n","\n","Effect: Generating interaction terms and higher-degree features for Age and Fare introduces quadratic relationships, allowing the model to capture more complex dependencies between these features.\n","\n","Modeling Impact: Polynomial features can improve the model’s ability to learn non-linear patterns, especially in cases where interactions between features are crucial to predictions.\n","\n","7. Power Transformation\n","\n","Purpose: Power transformations reduce skewness, normalize distributions, and stabilize variance, which can be beneficial for heavily skewed features.\n","\n","Effect: Applying a power transformation to Fare can make the distribution more Gaussian, which often improves performance, particularly for algorithms sensitive to skewed data.\n","\n","Modeling Impact: Normalizing distributions with power transformations can make learning more effective by reducing bias introduced by skewed data.\n","\n","8. Feature Selection\n","\n","Purpose: Feature selection helps identify the most relevant features, reducing dimensionality and minimizing noise.\n","\n","Effect: Using SelectKBest to choose the top features based on the ANOVA F-value selects features most related to the target variable (Survived), which reduces the dataset size and potentially improves model efficiency.\n","\n","Modeling Impact: Feature selection can lead to simpler, faster, and sometimes more accurate models by focusing on only the most predictive features, thereby improving model generalization and reducing overfitting.\n"],"metadata":{"id":"cDbj03Z7psfg"},"id":"cDbj03Z7psfg"},{"cell_type":"markdown","id":"2225fba5","metadata":{"id":"2225fba5"},"source":["## Summary\n","\n","In this assignment, you preprocessed the Titanic dataset by handling missing values, encoding categorical variables, scaling features, creating interaction terms with polynomial features, binarizing, discretizing, power transforming, and finally selecting the top features for the model input.\n","\n","This preparation is essential for ensuring that the data is well-suited for machine learning models."]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"],"id":"VHfHdGCP_n6Y"},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[],"id":"NMzKSbLIgFzQ"},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[],"id":"DjcH1VWSFI2l"},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[],"id":"4VBk_4VTAxCM"},{"cell_type":"code","metadata":{"id":"r35isHfTVGKc"},"source":["#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[],"id":"r35isHfTVGKc"},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[],"id":"XH91cL1JWH7m"},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[],"id":"z8xLqj7VWIKW"},{"cell_type":"code","metadata":{"id":"FzAZHt1zw-Y-","cellView":"form"},"source":["#@title Run this cell to submit your notebook for Ungrading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[],"id":"FzAZHt1zw-Y-"}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}