# Linear Classifier and Logistic Regression

## Overview
This session covers two important machine learning algorithms:
1. **Linear Classifier (Perceptron Model)** - for linearly separable classification
2. **Logistic Regression** - for probabilistic classification

## 1. Linear Classifier (Perceptron Model)

### Key Concepts
- **Definition**: A classification algorithm that draws a line to separate two classes when data is linearly separable
- **Mathematical Foundation**: Based on the abstraction function `y = f(w, x)`
- **Output**: Binary classification (0 or 1)

### Mathematical Formulation
```
y = f(w, x) = w₁x₁ + w₂x₂ + w₃x₃ + ... + bias
```

**Matrix Form**: `W^T × X = 0` (the separating line)

**Decision Rule** (Step Function):
- `y = 1` if `f(x) ≥ 0`
- `y = 0` if `f(x) < 0`

### Complexity Analysis
For D features:
- **Additions**: D-1 additions
- **Multiplications**: D multiplications

### Neural Network Connection
- Linear classifier mimics a single neuron
- Takes weighted inputs, applies activation function (step function)
- Foundation for understanding neural networks

### Practical Example - AND Gate
Implementation demonstrated using:
- Input: `X = [[0,0], [0,1], [1,0], [1,1]]`
- Output: `Y = [0, 0, 0, 1]`
- Successfully classified with linear separation

### Limitations
- Only works for linearly separable data
- Cannot handle complex patterns requiring curves or circles
- For non-linear data, need algorithms like Support Vector Machine (SVM)

## 2. Logistic Regression

### Key Concepts
- **Purpose**: Probabilistic classification using sigmoid function
- **Advantage**: Provides probability estimates, not just binary decisions
- **Relationship**: Combines linear regression with sigmoid activation

### Mathematical Foundation

**Linear Component**: `z = w₁x₁ + w₂x₂ + ... + bias`

**Sigmoid Function**: 
```
σ(z) = 1 / (1 + e^(-z))
```

**Decision Rule**:
- `y = 1` if `σ(z) ≥ 0.5`
- `y = 0` if `σ(z) < 0.5`

### Why "Logistic Regression"?
Despite being a classification algorithm, it's called "regression" because:
1. Uses linear regression to compute `z`
2. Applies sigmoid function to get probabilities
3. Historical naming convention

### Practical Implementation Examples

#### 1. Simple Binary Classification
- Same AND gate example as perceptron
- Achieved 75% accuracy (1 misclassification out of 4)

#### 2. Diabetes Dataset
- Binary classification (diabetic vs non-diabetic)
- Achieved 77% accuracy
- Compared with perceptron (70% accuracy)
- **Conclusion**: Logistic regression performed better

#### 3. Iris Dataset (Multi-class)
- Three classes: Setosa, Versicolor, Virginica
- Achieved 97% accuracy
- Demonstrates probability prediction capabilities
- Uses one-vs-rest approach for multi-class classification

### Key Features
1. **Probability Output**: Returns probability for each class
2. **Multi-class Support**: Can handle multiple classes using one-vs-rest
3. **No Scaling Impact**: Standard scaling doesn't significantly affect performance
4. **Iterative Training**: Uses maximum iterations parameter for optimization

## Code Implementation Highlights

### Libraries Used
```python
from sklearn.linear_model import Perceptron, LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import pandas as pd
import numpy as np
```

### Key Methods
- `model.fit(X_train, y_train)` - Train the model
- `model.predict(X_test)` - Make predictions
- `model.predict_proba(X_test)` - Get probability estimates
- `model.score(X_test, y_test)` - Calculate accuracy

## Comparison: Perceptron vs Logistic Regression

| Aspect | Perceptron | Logistic Regression |
|--------|------------|-------------------|
| Output | Binary (0/1) | Probabilities + Binary |
| Function | Step function | Sigmoid function |
| Best for | Linearly separable data | General binary classification |
| Multi-class | Limited | Good (one-vs-rest) |
| Probability estimates | No | Yes |

## When to Use Each Algorithm

### Linear Classifier (Perceptron)
- Data is perfectly linearly separable
- Simple binary classification
- When interpretability is crucial
- Fast computation needed

### Logistic Regression
- Need probability estimates
- Data may have some overlap between classes
- Multi-class classification
- More robust to outliers
- When you need confidence in predictions

## Key Takeaways

1. **Algorithm Selection**: Choose based on data characteristics and requirements
2. **Linear Separability**: Critical factor for perceptron success
3. **Probability vs Binary**: Logistic regression provides richer information
4. **Performance Varies**: Different algorithms work better on different datasets
5. **Multi-class Capability**: Logistic regression handles multiple classes more naturally

## Next Steps
- Explore Support Vector Machines for non-linear data
- Study Decision Trees for non-linear classification
- Learn about regularization techniques
- Understand ensemble methods for improved performance 