{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3A2XBqYQg-N"
   },
   "source": [
    "# Advanced Certification in AIML\n",
    "## A Program by IIIT-H and TalentSprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw4QHJlfsSF4"
   },
   "source": [
    "## Learning Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsSB95vPsoxS"
   },
   "source": [
    "At the end of the mini-hackathon you will be able to:\n",
    "* Perform Data preprocessing\n",
    "* Apply different ML algorithms on the **Titanic** dataset\n",
    "* Perform VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh70dVHx0G_B"
   },
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-GMJTRb0Iyy"
   },
   "source": [
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of many passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "[ Data Set Link: Kaggle competition](https://www.kaggle.com/competitions/titanic)\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Data Set Characteristics:\n",
    "\n",
    "**PassengerId:** Id of the Passenger\n",
    "\n",
    "**Survived:** Survived or Not information\n",
    "\n",
    "**Pclass:** Socio-economic status (SES)\n",
    "  * 1st = Upper\n",
    "  * 2nd = Middle\n",
    "  * 3rd = Lower\n",
    "\n",
    "**Name:** Surname, First Names of the Passenger\n",
    "\n",
    "**Sex:** Gender of the Passenger\n",
    "\n",
    "**Age:** Age of the Passenger\n",
    "\n",
    "**SibSp:**\tNo. of siblings/spouse of the passenger aboard the Titanic\n",
    "\n",
    "**Parch:**\tNo. of parents/children of the passenger aboard the Titanic\n",
    "\n",
    "**Ticket:**\tTicket number\n",
    "\n",
    "**Fare:** Passenger fare\n",
    "\n",
    "**Cabin:**\tCabin number\n",
    "\n",
    "**Embarked:** Port of Embarkation\n",
    "  * S = Southampton\n",
    "  * C = Cherbourg\n",
    "  * Q = Queenstown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmusUbENKSEt"
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "Build a predictive model that answers the question: “what sort of people were more likely to survive?” using titanic's passenger data (ie name, age, gender, socio-economic class, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ViFc50xKK-tY"
   },
   "outputs": [],
   "source": [
    "# @title Download the datasets\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "\n",
    "notebook=\"U1_MH1_Data_Munging\" #name of the notebook\n",
    "\n",
    "def setup():\n",
    "    from IPython.display import HTML, display\n",
    "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/titanic.csv\")\n",
    "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/test_titanic.csv\")\n",
    "    print(\"Data downloaded successfully\")\n",
    "    return\n",
    "\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RM8x-pMDLQuq"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5fBLdQaJtd4"
   },
   "source": [
    "## Exercise 1 - Load and Explore the Data (2 Marks)\n",
    "\n",
    "* Understand different features in the training dataset\n",
    "* Understand the data types of each column\n",
    "* Notice the columns of missing values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gn6HQH7abkyL"
   },
   "source": [
    "#### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4tUzpuurwCyH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sW9DMblWkhj_"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5z/k4qtwq8j41q4hbdpjmn5d3fh0000gn/T/ipykernel_61465/3523555977.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training dataset shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFirst 5 rows of training data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/vivek1929/machine-learning-generative-ai/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/vivek1929/machine-learning-generative-ai/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/vivek1929/machine-learning-generative-ai/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/vivek1929/machine-learning-generative-ai/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/vivek1929/machine-learning-generative-ai/.venv/lib/python3.11/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_csv('titanic.csv')\n",
    "print(\"Training dataset shape:\", train_df.shape)\n",
    "print(\"\\nFirst 5 rows of training data:\")\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54ktmr8lh8AS"
   },
   "outputs": [],
   "source": [
    "# Getting information about the dataset\n",
    "test_submission_df = pd.read_csv('test_titanic.csv')\n",
    "print(\"\\nTest dataset shape:\", test_submission_df.shape)\n",
    "print(\"\\nFirst 5 rows of test data:\")\n",
    "print(test_submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAvxH5MUPWDj"
   },
   "outputs": [],
   "source": [
    "print(\"=== TRAINING DATASET INFO ===\")\n",
    "print(\"Dataset Info:\")\n",
    "print(train_df.info())\n",
    "print(\"\\nDataset Description:\")\n",
    "print(train_df.describe())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nData Types:\")\n",
    "print(train_df.dtypes)\n",
    "print(\"\\nUnique values in each column:\")\n",
    "for col in train_df.columns:\n",
    "    print(f\"{col}: {train_df[col].nunique()} unique values\")\n",
    "    \n",
    "# Display survival distribution\n",
    "print(\"\\nSurvival Distribution:\")\n",
    "print(train_df['Survived'].value_counts())\n",
    "print(\"\\nSurvival Rate:\")\n",
    "print(train_df['Survived'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQGya6YLOku-"
   },
   "source": [
    "## Exercise 02: Split the data into train and test sets (1 Mark)\n",
    "Note: Apply all your data preprocessing steps in the train set first and keep the test set aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mps-O7zbPPcL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pMXWDyaspe3"
   },
   "source": [
    "## Exercise 03: Data Cleaning and Processing (15 Marks)\n",
    "### 3.1 Working on the \"Cabin\" column (2 Marks)\n",
    "Find unique entries in the Cabin column. We can label all passengers in two categories having a cabin or not. Check the data type(use: type) of each entry of the Cabin. Convert a string data type into '1' i.e. passengers with cabin and others into '0' i.e. passengers without cabin.  Write a function for the above operation and apply it to the cabin column and create another column with the name \" Has_cabin\" containing only 0 or 1 entries.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KozjEOo9VS3Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9N82uXWWf81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUm20kyHVTZe"
   },
   "source": [
    " ### 3.2 Working on \"SibSp\" & \"Parch\" columns (1 Mark)\n",
    "Combine columns \"SibSp\" & \"Parch\" and create another column that represents the total passengers in one ticket with the name \"family_size\". In each ticket, there might be Siblings/Spouses (SibSp =Number of Siblings/Spouses Aboard) or Parents/Children (Parch=Number of Parents/Children Aboard ) along with the passenger who booked the ticket.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2vHu13tWJx-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEkyLqqEVX2p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGPgnKttVYRL"
   },
   "source": [
    "### 3.3 Working on the\"Embarked\" column (2 Marks)\n",
    "The \"embarked\" column represents the port of Embarkation: Cherbourg(C), Queenstown(Q), and  Southampton(S ). Thus, the entries are of three categories in this column. Fill in the missing rows in this column. We can fill it with the most frequent category. Map these categorical string entries into numerical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IApw-rL1Y1jE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVTiJ27OVbzB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TFv47f9Y0yO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWmjUnN3VcF6"
   },
   "source": [
    "### 3.4 Working on the \"Age\" column (2 Marks)\n",
    "find the number of NaN entries in the age column and their row index. Calculate the mean, Standard deviation of the Age column and check the distribution of the age column.We can fill the missing values with randomly generated integer values between (mean+Standard deviation, mean-Standard deviation). Use : np.isnan; np.random.randint; concept of slicing dataframe. Convert the age column as an integer data type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbLqDB1hVf1a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFrug-3VakhY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9a69SMpa6aG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doeanDr0VgGV"
   },
   "source": [
    "### 3.5 Working on \"sex\" column (1 Mark)\n",
    "Map the Sex column as 'female' : 0, 'male': 1, and convert it into an integer data type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGN92EsEVlTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__CWnlGhVln2"
   },
   "source": [
    "### 3.6  Optional- Working on the \"Name\" column :\n",
    "Fetch titles from the name. We can map these titles with numbers and convert them into an integer. Use: concept of the regular expression.\n",
    "\n",
    "### 3.7 Optional- Working on the \"Fare\" column :\n",
    "We can convert face into categorical entries like Low, Medium, and High.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuuDE0mQVo7v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oJI0bQOVpP4"
   },
   "source": [
    "### 3.8 Drop the columns (1 Mark)\n",
    "\n",
    "Drop the columns: - \"PassengerId\", \"Name\",  \"SibSp\" & \"Parch\", \"Tickets\", \"Cabin\"\n",
    "\n",
    "Now apply different ML algorithms and check the accuracy of your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZlnDtiiVvif"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVYnt5p7ccKK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2EfoVojebWn"
   },
   "source": [
    "### 3.9 Apply Standard Scalar (1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVWd4PEaeiod"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kwa6Ua9Qgbi7"
   },
   "source": [
    "### 3.10 Create a single function for preprocessing the test set (X_test) and apply it. (4 Marks)\n",
    "#### **Note**: All the pre-processing steps that were applied on the train set before ML Modelling are also applied on the test set before passing through the predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLFNNM0SgqZ3"
   },
   "outputs": [],
   "source": [
    "## Create a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urGx3SRcc0kN"
   },
   "outputs": [],
   "source": [
    "## Applyting above function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlA3Gnmrc039"
   },
   "source": [
    "### 3.11 Apply standard Scalar transformation to x_test (1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0SAb9ccc2Qm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUMFQj-Gc1BO"
   },
   "source": [
    "## Exercise  4. Apply Multiple ML Algorithm and display the accuracy. (7 Marks)\n",
    " * Optional (  Ensemble Technique )\n",
    "#### Expected Accuracy >= 80%  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xd-21QgEc2TV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIf7BgedjLZ1"
   },
   "source": [
    "## Exercise  5. Pre-process the test_set (3 Marks)\n",
    "Again we have to apply the same preprocess function and standard scaler on this test set before passing through predict function.\n",
    "\n",
    "#### Understanding the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4ApkkLec2V7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syRBMp7ilrbe"
   },
   "source": [
    "#### Note: In the initial train set there were no missing entries in the \"Fare\" column. But, now for the submission test set, there is one missing entry in this column.\n",
    "\n",
    "#### There will be a minor change in the preprocess function to address the above issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppk5Fq0olrGF"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ4Lsp6znhrG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-TVJii2pwiI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mVUaw1um9hH"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-zATg3NnlKo"
   },
   "source": [
    "## Exercise  6. Prediction for test data (2 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvDpq5EHnkcB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
