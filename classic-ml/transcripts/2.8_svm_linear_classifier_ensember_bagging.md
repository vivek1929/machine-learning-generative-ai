# Machine Learning Session: SVM and Ensemble Methods (Bagging)

## Overview
This session covers two major topics in machine learning:
1. **Support Vector Machine (SVM)** - A linear classifier with margin optimization
2. **Ensemble Methods** - Specifically Bootstrap Aggregation (Bagging)

---

## Part 1: Support Vector Machine (SVM)

### What is SVM?
- **Support Vector Machine** is a classifier that separates data by finding the best line (hyperplane) with maximum margin
- **Support Vectors** are the data points that help identify the optimal separation line
- The algorithm takes help from support vectors to find the baseline that can separate two classes

### The Problem with Linear Classifiers
- Multiple lines can separate the same data perfectly
- Without proper selection criteria, any separating line might cause misclassification with new data
- **Solution**: Find the line with the largest margin to minimize future misclassification

### Key Concepts

#### Margin
- **Margin** is the distance between the decision boundary and the closest data points from each class
- Also called "no man's band" - no data points should lie within the margin
- **Larger margin** = better generalization = fewer misclassifications

#### Support Vectors
- Data points that lie on the margin boundaries
- These points "support" the decision boundary
- Used to calculate the optimal hyperplane

#### Mathematical Foundation
- **Decision boundary**: W^T × x + b = 0
- **Parallel hyperplanes**: 
  - W^T × x + b = +1 (positive class boundary)
  - W^T × x + b = -1 (negative class boundary)
- **Margin calculation**: 2 / ||W|| (distance between parallel hyperplanes)

### SVM Parameters

#### C Parameter (Regularization)
- **Hard Margin (C = 1000+)**: Forces perfect separation, no misclassification allowed
- **Soft Margin (C = 0.1)**: Allows some misclassification for better generalization
- **Trade-off**: High C may lead to overfitting, low C may underfit

#### Kernel Parameter
- **Linear kernel**: For linearly separable data
- **Polynomial kernel**: For non-linear data (covered in Unit 2)
- **RBF kernel**: For complex non-linear patterns (covered in Unit 2)

### Implementation Example
```python
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

# Create SVM classifier
clf = SVC(kernel='linear', C=1000)

# Train the model
clf.fit(X_train, y_train)

# Get support vectors
print(clf.support_vectors_)

# Get model coefficients and intercept
print(clf.coef_)      # W1, W2
print(clf.intercept_) # bias term
```

---

## Part 2: Ensemble Methods - Bagging

### What is Ensemble?
- **Ensemble** means combining more than one algorithm together
- Goal: Improve prediction accuracy by aggregating multiple models

### Bootstrap Aggregation (Bagging)

#### Bootstrap Process
1. **Random sampling with replacement** from training data
2. Create multiple new datasets (bags) of the same size as original
3. Each bag contains ~63.2% unique samples + ~36.8% duplicates
4. **Mathematical foundation**: Probability of not selecting = (1 - 1/n)^n ≈ 36.8%

#### Why Use Replacement?
- **Without replacement**: All bags would be identical → no diversity
- **With replacement**: Creates variance in data → reduces overfitting
- **Purpose**: Make algorithm understand small fluctuations in data

#### Aggregation Process
1. Train the same algorithm on each bag
2. Make predictions using all trained models
3. **Final prediction**: Average of all model predictions (for regression) or majority vote (for classification)

### Types of Ensemble Methods

#### 1. Bagging Classifier
```python
from sklearn.ensemble import BaggingClassifier
from sklearn.svm import SVC

# Create bagging classifier with SVM
clf = BaggingClassifier(
    base_estimator=SVC(kernel='linear'),
    n_estimators=10,
    random_state=42
)

clf.fit(X_train, y_train)
```

- Can use **any algorithm** as base estimator
- Creates specified number of bags
- Applies same algorithm to each bag

#### 2. Random Forest Classifier
```python
from sklearn.ensemble import RandomForestClassifier

# Random Forest (bagging with decision trees)
rf = RandomForestClassifier(
    n_estimators=20,
    random_state=42
)

rf.fit(X_train, y_train)
```

- **Fixed algorithm**: Only uses Decision Trees
- Special case of bagging classifier
- Default choice for many classification problems

#### 3. Voting Classifier
```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# Create ensemble with different algorithms
estimators = [
    ('lr', LogisticRegression(max_iter=300)),
    ('svc', SVC(kernel='linear')),
    ('dt', DecisionTreeClassifier())
]

voting_clf = VotingClassifier(
    estimators=estimators,
    voting='hard'  # or 'soft'
)

voting_clf.fit(X_train, y_train)
```

#### Voting Types
- **Hard Voting**: Majority vote based on predicted classes
- **Soft Voting**: Average of predicted probabilities (generally better)

### Benefits of Bagging
1. **Reduces overfitting** by training on diverse datasets
2. **Reduces variance** in predictions
3. **Improves generalization** to unseen data
4. **Handles small fluctuations** in data better

---

## Practical Implementation

### Dataset Used
- **Diabetes dataset** (768 samples, 8 features)
- Binary classification problem
- Train-test split: 80-20

### Results Comparison
- **SVM with Bagging**: ~78% accuracy
- **Logistic Regression with Bagging**: ~74% accuracy
- **Voting Classifier**: ~75% accuracy

### Key Insights
1. **SVM performed better** than Logistic Regression on this dataset
2. **Bagging helps avoid overfitting** especially for algorithms prone to it (like Decision Trees)
3. **Voting classifier** provides expert committee approach
4. **No single best algorithm** - depends on data characteristics

---

## Important Notes

### When to Use SVM
- Data is linearly separable or can be made linearly separable
- Need clear margin between classes
- Want to avoid overfitting
- Binary or multi-class classification problems

### When to Use Bagging
- Algorithm is prone to overfitting (especially Decision Trees)
- Want to reduce variance in predictions
- Have sufficient computational resources
- Need more stable and reliable predictions

### Hyperparameter Tuning
- Use **Grid Search CV** or **Optuna** for optimal parameter selection
- Test different values of C for SVM
- Experiment with number of estimators for ensemble methods
- Consider using **Random Search** for large parameter spaces

---

## Next Steps
- **Unit 2**: Non-linear SVM (Polynomial and RBF kernels)
- **Advanced Ensemble Methods**: Boosting techniques
- **Real-world Applications**: Deployment and production considerations
- **Hyperparameter Optimization**: Automated ML approaches

---

## Q&A Highlights

### Common Questions Addressed:
1. **Why use margin in SVM?** - To minimize future misclassification
2. **Why replacement in bagging?** - To create diversity and reduce overfitting
3. **Difference between bagging and voting?** - Bagging uses same algorithm, voting uses different algorithms
4. **Hard vs Soft margin?** - Trade-off between perfect separation and generalization
5. **When to use ensemble?** - When single models are prone to overfitting or you need more stable predictions 