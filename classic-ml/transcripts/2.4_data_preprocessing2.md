# Data Preprocessing - Session 2.4 Summary

## Session Overview
This session covers the fundamentals of data preprocessing in machine learning, focusing on practical implementation using pandas and scikit-learn libraries.

## Agenda
1. **Data Ingestion** - Reading and loading data into usable formats
2. **Data Preprocessing** - Cleaning and preparing data for machine learning
3. **Exploratory Data Analysis (EDA)** - Understanding data characteristics
4. **Scaling Techniques** - Normalizing data for better model performance
5. **Cloud Integration** - Brief overview of database connections

## Key Concepts Covered

### 1. Data Frames in Pandas

#### What is a Data Frame?
- **Definition**: A two-dimensional table in Python used for organizing and analyzing data
- **Structure**: Consists of rows (individual entries) and columns (attributes)
- **Key Features**:
  - **Structure**: Each column can hold different data types (numbers, text, dates)
  - **Indexing**: Rows have labels/indexes for easy access
  - **Functionality**: Supports filtering, manipulation, statistics, and data merging
  - **Efficiency**: Built for handling large datasets (thousands to millions of rows)

#### Creating Data Frames
```python
import pandas as pd

# From CSV file
df = pd.read_csv('iris.csv')

# From sklearn datasets
from sklearn.datasets import load_iris
iris = load_iris()
df_iris = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df_iris['species'] = iris.target
```

#### Accessing Data Frame Elements
Three methods to access columns:
1. **Dot notation**: `df.column_name` (works only for single words)
2. **Bracket notation**: `df['column_name']` (works with spaces)
3. **iloc method**: `df.iloc[:, column_index]` (most flexible, allows slicing)

### 2. Basic Data Frame Operations

#### Essential Commands
```python
# Basic information
df.info()           # Data types and non-null counts
df.dtypes          # Data types only
df.head()          # First 5 rows
df.head(15)        # First 15 rows
df.describe()      # Descriptive statistics
df.shape           # Dimensions (rows, columns)

# Accessing specific data
df['column_name'].unique()    # Unique values in column
df['column_name'].mean()      # Mean of column
```

### 3. Exploratory Data Analysis (EDA)

#### Using ydata-profiling
```python
# Installation
pip install ydata-profiling

# Usage
from ydata_profiling import ProfileReport

profile = ProfileReport(df_iris, 
                       title="Pandas Profiling Report for Iris Data",
                       explorative=True)
profile.to_file("iris_pandas_profiling.html")
```

#### EDA Benefits
- **Comprehensive Overview**: Variables, observations, missing cells, duplicates
- **Data Quality Assessment**: Missing values, data types, distributions
- **Statistical Insights**: Mean, median, quartiles, correlations
- **Visual Analysis**: Histograms, correlation matrices
- **Alerts**: Highly correlated features, skewed distributions

### 4. Data Preprocessing Steps

#### Step-by-Step Process

##### Step 1: Data Ingestion
```python
# Reading data with custom separators and column names
df = pd.read_csv('auto_mpg.data', 
                 sep=r'\s+',  # Multiple spaces separator
                 names=column_names,
                 na_values=['?'])  # Treat '?' as null values
```

##### Step 2: Data Type Verification
```python
df.dtypes  # Check if data types are correct
df.info()  # Comprehensive information
```

##### Step 3: Handle Missing Values
```python
# Identify missing values
df.isna().sum()

# Remove missing values
df.dropna(inplace=True)

# Alternative: Impute missing values
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
df_imputed = imputer.fit_transform(df_clean)
```

##### Step 4: Remove Unwanted Columns
```python
# Drop text columns for numerical analysis
df_clean = df.drop('car_name', axis=1)
```

##### Step 5: Data Scaling
ðŸ“º **Video Tutorial**: [Why data scaling required](https://www.youtube.com/watch?v=sxEqtjLC0aM)

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df_clean), 
                        columns=df_clean.columns)
```

**Standard Scaling Formula**: `z = (x - Î¼) / Ïƒ`
- Where Î¼ = mean, Ïƒ = standard deviation

##### Step 6: Label Encoding (for categorical data)
```python
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df['gender_encoded'] = encoder.fit_transform(df['gender'])
```

##### Step 7: Feature Selection
```python
from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(score_func=f_regression, k=5)
X_selected = selector.fit_transform(X, y)

# Get selected feature names
selected_features = X.columns[selector.get_support()]
```

### 5. Outlier Detection

#### Mathematical Approach (IQR Method)
```python
# Calculate quartiles
Q1 = df['column'].quantile(0.25)
Q3 = df['column'].quantile(0.75)
IQR = Q3 - Q1

# Define bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Find outliers
outliers = df[(df['column'] < lower_bound) | (df['column'] > upper_bound)]
```

#### Using Machine Learning Libraries
```python
# Isolation Forest
from sklearn.ensemble import IsolationForest

iso_forest = IsolationForest(contamination=0.1, random_state=42)
df['outlier'] = iso_forest.fit_predict(df)

# Local Outlier Factor
from sklearn.neighbors import LocalOutlierFactor

lof = LocalOutlierFactor(n_neighbors=5, contamination=0.1)
outlier_labels = lof.fit_predict(df)
```

### 6. Complete Preprocessing Pipeline

#### Industry Standard Steps:
1. **Data Ingestion**: Load data into DataFrame
2. **Data Inspection**: Check data types and structure (`df.info()`)
3. **Data Type Conversion**: Fix mismatched data types
4. **Missing Value Handling**: Identify and handle null values (`df.isna().sum()`)
5. **Categorical Encoding**: Convert text to numerical (Label Encoder)
6. **Data Scaling**: Normalize numerical features (Standard Scaler)
7. **Outlier Detection**: Identify and handle outliers
8. **Feature Selection**: Select most relevant features
9. **Algorithm Implementation**: Apply machine learning algorithms

### 7. Important Guidelines

#### Missing Value Handling
- **Rule of Thumb**: If less than 5% of data is missing, consider removal
- **If more than 5%**: Use imputation techniques (mean, median, mode)

#### Data Quality Checks
- Verify data types match expected formats
- Check for special characters or noise in data
- Validate data ranges and logical consistency
- Look for duplicate records

#### Scaling Considerations
- **When to scale**: When features have different units or ranges
- **Types of scaling**:
  - Standard Scaler: Mean = 0, Std = 1
  - Min-Max Scaler: Scale to [0,1] range
  - Robust Scaler: Uses median and IQR

### 8. Code Examples and Best Practices

#### Reading Data with Issues
```python
# Handle multiple separators and missing values
df = pd.read_csv('data.csv', 
                 sep=r'\s+',           # Multiple whitespaces
                 na_values=['?', 'N/A', ''],  # Multiple null indicators
                 names=column_names)    # Custom column names
```

#### Data Quality Assessment
```python
# Quick data overview
print(f"Shape: {df.shape}")
print(f"Data types:\n{df.dtypes}")
print(f"Missing values:\n{df.isna().sum()}")
print(f"Duplicates: {df.duplicated().sum()}")
```

#### Preprocessing Pipeline Example
```python
# Complete preprocessing pipeline
def preprocess_data(df):
    # 1. Handle missing values
    df = df.dropna()
    
    # 2. Remove text columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df_clean = df[numeric_cols]
    
    # 3. Scale features
    scaler = StandardScaler()
    df_scaled = pd.DataFrame(scaler.fit_transform(df_clean), 
                            columns=df_clean.columns)
    
    return df_scaled
```

## Key Takeaways

1. **Data Quality is Crucial**: 60-80% of data science effort goes into preprocessing
2. **Understanding Your Data**: Use EDA tools like ydata-profiling for quick insights
3. **Systematic Approach**: Follow the 9-step preprocessing pipeline
4. **Tool Familiarity**: Master pandas operations and sklearn preprocessing tools
5. **Domain Knowledge**: Outlier handling depends on business context
6. **Documentation**: Always refer to official sklearn documentation for best practices

## Tools and Libraries Used

- **pandas**: Data manipulation and analysis
- **sklearn**: Machine learning preprocessing tools
- **ydata-profiling**: Automated EDA reports
- **numpy**: Numerical operations
- **Google Colab**: Development environment

## Next Steps

The next session will cover implementing machine learning algorithms (starting with linear regression) on preprocessed data, building upon the preprocessing techniques learned in this session. 