# Session 2.5: Linear Regression & K-Nearest Neighbors (KNN)

## Overview
This session covers two fundamental machine learning algorithms:
1. **Linear Regression** (First half) - Regression problem
2. **K-Nearest Neighbors (KNN)** (Second half) - Classification problem

---

## Part 1: Linear Regression

### Key Concepts
- **Goal**: Find the best fit line through data points
- **Mathematical Form**: `y = mx + c`
  - `m` = slope (how much y increases for each unit increase in x)
  - `c` = y-intercept (where line crosses y-axis)

### Mathematical Intuition
**Slope Formula**:
```
m = [n∑(xiyi) - ∑(xi)∑(yi)] / [n∑(xi²) - (∑xi)²]
```

**Intercept Formula**:
```
c = [∑yi - m∑xi] / n
```

### Key Characteristics
- **Closed Form Solution**: Calculates best fit line directly using mathematical formulas
- **Not Iterative**: Unlike gradient descent, doesn't learn row by row
- **Best Fit Line**: Line that passes through most data points with minimum error

### Implementation Steps
1. Import required libraries (`sklearn.linear_model.LinearRegression`)
2. Read data into DataFrame
3. Preprocess data (handle null values, scaling)
4. Define features (X) and target (y)
5. Apply Standard Scaler
6. Split data (train/test)
7. Create model object
8. Fit the model
9. Evaluate the model

### Example Implementation
```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Create and train model
model = LinearRegression()
model.fit(X_train, y_train)

# Get slope and intercept
print(f"Slope: {model.coef_}")
print(f"Intercept: {model.intercept_}")

# Evaluate
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)
```

### Real-world Example
- **Dataset**: Auto MPG dataset
- **Task**: Predict miles per gallon based on car features
- **Results**: Achieved ~80% accuracy on both training and test data

---

## Part 2: K-Nearest Neighbors (KNN)

### Core Concept
> "Birds with similar feathers fly together"

- **Classification**: Assigns class based on majority vote of K nearest neighbors
- **Regression**: Assigns average value of K nearest neighbors

### Key Characteristics
- **Lazy Learning Algorithm**: Doesn't learn patterns, just memorizes data locations
- **Memory Intensive**: Stores all training data
- **Distance-based**: Uses distance metrics to find neighbors

### Distance Metrics
1. **Euclidean Distance**: `√[(x₂-x₁)² + (y₂-y₁)²]`
2. **Manhattan Distance**: `|x₁-x₂| + |y₁-y₂|`
3. **Minkowski Distance**: Generalized form
   - When p=1: Manhattan distance
   - When p=2: Euclidean distance
4. **Hamming Distance**: For binary data only

### Important Rules
- **K should be odd**: Avoids ties in majority voting
- **K typically ≤ 30**: Beyond 30, consider other algorithms
- **Always apply scaling**: Essential due to distance-based nature

### Algorithm Process
1. **Fit Phase**: Memorize all training data locations
2. **Predict Phase**: 
   - Calculate distances to all training points
   - Find K nearest neighbors
   - For classification: Take majority vote
   - For regression: Take average value

### Implementation Steps
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

# Apply scaling (crucial for KNN)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create and train model
knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')
knn.fit(X_train, y_train)

# Predict and evaluate
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
```

### Evaluation Metrics
- **Classification**: Accuracy, Confusion Matrix
- **Confusion Matrix**: Shows correct vs incorrect predictions
- **Accuracy**: Diagonal elements / Total predictions

### When to Use KNN
**Good for**:
- Small datasets
- Few features
- Non-linear relationships

**Avoid when**:
- Large datasets (memory intensive)
- Many features (curse of dimensionality)
- Need fast predictions

---

## General ML Workflow (Used for Both Algorithms)

1. **Import Libraries**: Import required sklearn modules
2. **Data Ingestion**: Read data into pandas DataFrame
3. **Data Preprocessing**: 
   - Handle missing values
   - Apply scaling (StandardScaler)
   - Label encoding for categorical variables
4. **Feature Selection**: Define X (features) and y (target)
5. **Data Splitting**: Train-test split
6. **Model Creation**: Create algorithm object
7. **Model Training**: Fit model on training data
8. **Model Evaluation**: Test on unseen data

---

## Key Takeaways

### Linear Regression
- Use when relationship between features and target is linear
- Good interpretability (slope and intercept have clear meaning)
- Fast training and prediction
- Baseline algorithm for regression problems

### KNN
- Simple, intuitive algorithm
- No assumptions about data distribution
- Can capture complex non-linear relationships
- Performance depends heavily on choice of K and distance metric
- Requires feature scaling

### Model Selection
- **80%+ accuracy**: Generally acceptable for production
- **Poor performance**: Try different algorithms
- **Future tools**: AutoML, Optuna for automated algorithm selection
- **Hyperparameter tuning**: Grid Search CV for optimal parameters

---

## Tools and Libraries Mentioned
- **scikit-learn**: Main ML library
- **pandas**: Data manipulation
- **numpy**: Numerical computations
- **Gradio**: Quick model deployment for POCs
- **Optuna**: Hyperparameter optimization
- **AutoML**: Automated machine learning
- **UCI Repository**: Machine learning datasets

---

## Next Steps
- Learn more classification algorithms (Decision Trees, Logistic Regression)
- Feature engineering techniques
- Hyperparameter optimization
- Model deployment strategies 